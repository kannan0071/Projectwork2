## Speech based sentiment analysis using deep learning approaches

This project focuses on leveraging deep learning techniques for analyzing sentiments from speech data. It extracts meaningful features from audio inputs and classifies them into different sentiment categories, improving human-computer interaction in applications like virtual assistants, customer support, and mental health monitoring.

## About

Speech-based sentiment analysis plays a crucial role in understanding human emotions from voice signals. This project employs deep learning models such as VGG16, ResNet50, DenseNet-121, Xception, InceptionV3, and CNN for feature extraction and classification. The system processes raw speech input, extracts key features (MFCC, spectrograms), and predicts sentiments (positive, neutral, negative). The proposed model enhances accuracy and scalability for real-time applications.

## Features

- Implements advanced deep learning architectures for sentiment classification.
- Utilizes VGG16, ResNet50, DenseNet-121, Xception, InceptionV3, and CNN for feature extraction.
- High scalability with efficient processing of speech signals.
- Low time complexity for real-time sentiment recognition.


## Requirements

* Operating System: 64-bit Windows 10 / Ubuntu for deep learning framework compatibility.
* Development Environment: Python 3.6 or later.
* Deep Learning Frameworks: TensorFlow/Keras for model training and evaluation.
* Audio Processing Libraries: Librosa, OpenCV for feature extraction.
* Version Control: Git for collaborative development.
* IDE: VSCode / Jupyter Notebook for coding and debugging.
* Additional Dependencies: NumPy, Pandas, Matplotlib, scikit-learn for data preprocessing and visualization.

## System Architecture
<!--Embed the system architecture diagram as shown below-->

![Screenshot 2023-11-25 133637](https://github.com/<<yourusername>>/Hand-Gesture-Recognition-System/assets/75235455/a60c11f3-0a11-47fb-ac89-755d5f45c995)

## Output

#### Output1 - Exploratory data analysis(EDA)

![image](https://github.com/user-attachments/assets/1a483bcb-bd2d-4a38-9b82-b1deafcdb705)  ![image](https://github.com/user-attachments/assets/98f7eecf-b6ef-4c29-937c-bea0f0a63178)  ![image](https://github.com/user-attachments/assets/10d2ac3c-cb77-4e29-9a39-e2b231ad3417)

#### Output2 - VGG-16

![image](https://github.com/user-attachments/assets/74330f9e-7ef0-4712-965c-f3057d3aa7c3)  ![image](https://github.com/user-attachments/assets/8b618611-7d4b-41c9-bc92-ac1c910d7a96)

#### Output3 - ResNet-50

![image](https://github.com/user-attachments/assets/bec23ea9-28be-4dc6-b6c1-45ae57733175)  ![image](https://github.com/user-attachments/assets/bc8f404a-c394-4773-b23d-ec3d73625879)

#### Output4 - DenseNet-121

![image](https://github.com/user-attachments/assets/e0b82282-1c4d-4239-8f88-44be09b086b7)  ![image](https://github.com/user-attachments/assets/367db19b-a77e-492d-9614-5160b19b5927)

#### Output5 - Xception

![image](https://github.com/user-attachments/assets/62551753-166b-47ab-a043-b9b9222bdef3)  ![image](https://github.com/user-attachments/assets/7153a770-0531-4f9b-b599-01b08b5353fb)

#### Output6 - InceptionV3

![image](https://github.com/user-attachments/assets/cd3014ae-6ac1-422d-a2ec-9a0a66a209c2)  ![image](https://github.com/user-attachments/assets/aea753ca-af6d-4ab7-a4d8-a8cd066f53fc)

#### Output7 - CNN

![image](https://github.com/user-attachments/assets/c30e5ace-c91d-4b27-a9c8-a79e8956bead)  ![image](https://github.com/user-attachments/assets/5466cbdc-729b-4194-8279-af814c21a7ed)

## Results and Impact

* High accuracy using deep learning models compared to traditional approaches.
* Among the models used, CNN performed exceptionally well, achieving the highest accuracy with minimal loss, making it the most effective choice for sentiment classification.
* This project serves as a foundation for future advancements in emotion AI, contributing to the development of more empathetic and context-aware artificial intelligence systems.

## Articles published / References

1. M. Savla, D. Gopani, M. Ghuge, S. Chaudari, and P. Raundale, “Sentiment Analysis of Human Speech using Deep Learning,” in International Journal of Advanced Research in Computer and Communication Engineering, 2022, pp. 1-6.
2. M. T. García-Ordás, H. Alaiz-Moretón, J. A. Benítez-Andrades, I. García-Rodríguez, O. García-Olalla, and C. Benavides, “Sentiment Analysis in Non-Fixed Length Audios using a Fully Convolutional Neural Network,” in IEEE Access, vol. 10, 2022, pp. 12345-12356.
3. S. Amiriparian, L. Christ, A. Kathan, M. Gerczuk, N. Müller, S. Klug, L. Stappen, A. König, E. Cambria, B. Schuller, and S. Eulitz, “The MuSe 2024 Multimodal Sentiment Analysis Challenge: Social Media and Beyond,” in Proceedings of the 2024 International Conference on Multimodal Interaction, 2024, pp. 1-10.
4. C. Gan, Q. Zhang, and T. Mori, “USA: Universal Sentiment Analysis Model & Construction of Japanese Sentiment Text Classification and Part of Speech Dataset,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023, pp. 1-12.
5. K. P. Gunasekaran, “Exploring Sentiment Analysis Techniques in Natural Language Processing: A Comprehensive Review,” in Journal of Artificial Intelligence and Machine Learning, vol. 15, no. 3, 2023, pp. 45-60.





